
\section*{Materials and Methods}

Let  $\bm{b_{j}}$ represents the genetic effect of SNP-gene pair $j$ across $R$ = 44 tissues.

We assume the following mixture prior for the $R$ dimensional vector of true effects,  

 \begin{equation}
  \bm{b_{j}} | \bm{\pi},\bf{U}, \bm{\omega} \sim \sum_{k,l} \pi_{k,l} \;{\it N}_R(x; \bm{0}, \omega_l U_{k})
\end{equation}

Where ${\it N}_R(.; \bm{0}, \omega_l U_{k})$ denotes the density of a normal distribution with mean $\bm{0}$ and variance $\omega_l U_{k}$.


%As mentioned above, 
Each component of the mixture distribution is characterized by these prior covariance matrices, $U_{k}$ which capture the pattern of effects across tissues. Critically, this prior distribution is the same for all $J$ - hence the hierarchical incorporation of shared information.

\subsection{Covariance Matrices}\label{sssec:num1}

For a given $\omega_{l}$, we specify 4 `types' of $RxR$ prior covariance matrices $U_{k,l}$.
\begin{enumerate}

\item $U_{k=1,l}$ = $\omega_l$ $\mathbf{I}_{R}$

\item $U_{k=2,l}$ = $\omega_l$X_{z}$ The (naively) estimated tissue covariance matrix as estimated from the column-centered J \times R$ matrix of $Z$ statistics, $Z_{center}$: $\frac{1}{J}$ $Z_{center}$^{t}$ $Z_{center}$

\item $U_{k=3,l}$ = $\omega_l$ $\frac{1}{J}$ $V_{1...p}$ $d^{2}_{1...p}$   $V^{t}_{1..p}$ is the rank $p$ eigenvector approximation of the tissue covariance matrices, i.e., the sum of the first $p$ eigenvector approximations, where $\pcv_{1...p}$  represent the eigenvectors of the covariance matrix of tissues and $\pcd_{1...p}$ are the first $p$ eigenvalues.

\item $U_{k=4:4+Q-1,l}$ = $\frac{1}{J}(($\Lambda\mathbf{F})^{t} \Lambda \mathbf{F})_{q}$ corresponding to the $q_{th}$ sparse factor representation of the tissue covariance matrix %(not the sum of the first $q$, as above)

\item $U_{k=4+Q,l}$ = $\frac{1}{J}$ ($\($\Lambda \mathbf{F})^{t} \Lambda \mathbf{F}$ is the sparse factor representation of the tissue covariance matrix, estimated using all $q$ factors.



\item $U_{k=5+Q:R+4+Q,l}$ = $\frac{1}{J}$ $([1 0 0 . . ]'[1 0 0  . . .])$ %is the sparse factor representation of the tissue covariance matrix, estimated using all $q$ factors.
\item $U_{k=R+5+Q,l}$ = $\frac{1}{J}$ $([1 1 1 . . .]'[1 1 1 . . .])$
\item $[1 0 0 0 ...]$ or $[1 1 1  ...]$ represent configurations such that given membership,$\bm{b_{j}}$ arise from the same prior variance.
\end{itemize}

Critically, the computations above are estimated using the strongest snp per gene as characterized by the maximum absolute value across R tissues, in order to optimally initialize a `denoised' matrix of the true effects. 


\subsection{Deconvolution}
To retrieve a `denoised' or `deconvoluted' estimate of the non-single rank dimensional reduction matrices, we then perform deconvolution after initializing the EM algorithm with  the matrices specified in (2), (3) and (5). The final results of this iterative procedure preserves the rank of the initialization matrix, and allows us to use the `true' effect at each component component $\bm{{b}_{j}}$ as missing data in deconvoluting the prior covariance matrices. In brief, this algorithm works by treating not only the component identity but also the true effect $\bm{{b}_{j}}$  as unobserved data, and maximizing the likelihood over the expectation of the complete data likelihood, considering the values $\bm{{b}_{j}}$ as extra missing data (in addition to the indicator variables $q_{ij}$) (Bovy et al, 2014). 
%
%This allows us to write down the `full data' log likelihood as follows:
%
%\begin{equation}
%\begin{center}
%\begin{aligned}
%\phi=\sum_{J} \sum_{K} q_{jk} ln \alpha_{k} \it{N}(\hat{\bm{{b}_{j}}}|0,U_{k}+V_{j})\\
%\phi=\sum_{J} \sum_{K} q_{jk} ln \alpha_{k} \it{N}(\bm{{b}_{j}}|0,U_{k})
%\end{aligned}
%\end{center}
%\end{equation}
%
%
%Where $\alpha_{k}$ represents $\pi_k$ and $q_{jk}$ is the latent identifier variable.

%
%\subsection{Generation of List of Covariance Matrices}
%
%We then use these three non single-rank covariance matrix in place of our original choice of the empirical covariance matrix, SFA and SVD approximations. Here, I also used the Identity (K=1), 5 single-rank SFA factors (K=4-9), and the 44+1 $eqtlbma.lite$ configurations (K=10:54) in steps (7) and (8) to a assemble a full list of covariance matrices. Briefly, these  $eqtlbma.lite$ are an attempt to capture 'singleton' and 'fully shared' configurations in which the gene-snp pair is active in only one or all tissues. In the latter case, the variance of the distribution of underlying effect sizes is equal in all tissues.  This is 54 matrices, and we then proceed to chooses an $'L'$ element grid according to the range of effect sizes present in the overall data set in order to create a KxL list of covariance matrices. In the GTeX data set described in the text, we choose a grid with 22 $\omega$ for a total of 1188 covariance matrices.

Let us concatenate the list of all KxL combinations of prior covariance matrices $U_{k}$ and their scaling parameters $\omega_{l}$ into a KxL list and assign this length $P$ for simplicity of notation.

Now each $U_{p}$ imparts information about both $\emph{scale}$ and $\emph{direction}$

\subsection{Mixture Weights: Estimate $\hat{\bm{\pi}}$}
We wish to choose the model which best maximizes the probability of observing the data set. First, we must estimate the prior mixture weights $\pi$ by maximizing the likelihood 

\begin{equation}
L(\pi;\hat{\bm{b}}, \bm{s}) := p(\hat{\bm{b}}, \bm{s} | \pi) 
\label{eq:cdll}
\end{equation}

We express the marginal (observed) data likelihood for a finite mixture (Laird \emph{et al}, 1977):
%Here, the total likelihood of the test data set over $K$ components: 

\begin{equation}
L(\bm\pi;{\hat{\bm{b}}},\bm{s}) = \prod_{j=1}^J \sum_{p}^{P} \pi_{p} P(\hat{\bm{b_{j}}}, \bm{s} | z_{j}=p)
\label{eq:pihat}
\end{equation}

Here, in order to obtain mixture weights $\bm{\pi}$ which reflect the abundance of each pattern of sharing in the overall data set, we use a random set of gene-snp pairs (i.e., not restricting our analysis to the pairs with the strongest Z statistics used in the section above) and obtain the maximum likelihood estimates of the mixture weights, $\hat{\bm{\pi}}$, which maximize the complete data likelihood $\ref{eq:cdll}$ (Laird 1977; Barut 2005).

\begin{itemize}
\item  To estimate the hierarchical prior weights $\hat{\bm{\pi}}$ we compute the likelihood at each of these randomly chosen gene-snp pairs $j$ by evaluating the probability of observing $\bm{\hat{b}_{j}}$ given that we know the true $\bm{b_{j}}$ arises from component $\bm{p}$ (see \ref{eqn:new_lik}).
\item  Use the Expectation Maximization (EM) algorithm to estimate the optimal combination of weights using the  $\textbf{JxP}$ matrix of likelihoods computed according to ($\ref{eqn:new_lik}$).
\end{itemize}

%We then use these weights to estimate the test set log-likelihood.

\subsection{Likelihood}

By maximum likelihood in each tissue separately, we can easily obtain the observed estimates of the standardized genotype effect sizes, $\hat{\bm{b}}_{j}$, and their observed squared standard errors recorded on the diagonal of an $R \times R$ matrix denoted $\hat{V}_{j} = \Var(\hat{\bm{b}}_{j})$. 
We assume that the matrix of standard errors of $\hat{\bm{b}}_{j}$, $V_{j}$ as approximated by $\hat{V_{j}}$ is diagonal and  that $\hat{V}_{j}$ is an accurate point estimate for the standard error and that these standard errors are independent between tissues.

If we now view $\hat{\bm{b}}_{j}$ and $\hat{V}_{j}$ as \emph{observed data}, we can write a new ``likelihood'' using only the sufficient statistics,   $\hat{\bm{b}}_{j}$ and $\hat{V}_{j}$:

\begin{equation}
\hat{\bm{b}}_{j} | \bm{b}_{j} \sim \Norm_R(\bf{x}; \bm{b}_{j}, \hat{V}_{j})
    \label{eqn:new_lik}
\end{equation}

In brief, though we report the analysis using $\hat{\bm{b}}_{j}$ and its standard error, inference can be similarly performed on the vector of observed Z statistics $\bm{Z}_{j}$ with the matrix of squared Standard Error $\hat{V}_{j}$ thus defined as the RxR Identity Matrix.

\subsection{Posterior Quantities}\label{sssec:posteriors}

Armed with the prior mixture weights stored in the $P$ vector $\hat{\bm{\pi}}$ we proceed to the inference step and compute the posterior weights ($\ref{eqn:postpi}$) and corresponding posterior quantities across all original 16,069 gene-snp pairs. %In brief, the posterior mean and tissue specific tail probabilities are computed across all K components for each gene snp pair, and then weighted according to the posterior weights. This is performed in the $\textbf{weightedquants}$ step.\includegraphics[]{Methods.pdf}

We aim to report posterior quantities for a given gene-snp pair $\textbf{j}$. We know that for a single multivariate {\it Normal}  the posterior on  $\bm{b} | U$ is  simply: 

\begin{equation}
\[
\bm{b} | \hat{\bm{b}} \sim {\it N}_R(\bm{\tilde{\mu}}, \tilde{U})
\]
\end{equation}

where:
\begin{itemize}
\item $\bm{\tilde{\mu}}= \tilde{U}(\hat{V}^{-1} \hat{\bm{b}})$
\item $ \tilde{U} = ({U}^{-1} + \hat{V}^{-1})^{-1}$.
\end{itemize}


. 
Furthermore, a mixture-multivariate normal prior and a normal likelihood yields a mixture multivariate posterior, where the final posterior distribution is simply a weighted combination of multivariate normal distributions, where for each gene-snp pair $\textbf{j}$ is now characterized by it's posterior mean $\tilde{\bm{\mu}}_{jp}$ and covariance  $\tilde{U}_{jp} = (U_{p}^{-1} + \hat{V}_{j}^{-1})^{-1}$.

\begin{equation}
\begin{aligned}
  \label{eq:mixpost}
\bm{b}_{j} | \hat{\bm{b}_{j}}, \hat{V}_{j}, \hat{\bm{\pi}} 
%&= \sum_{k=1,l=1}^{K,L} \sim {\it N}_R(\bm{\mu}_{1kl}, U_{1kl})%p(\bm{b}_{j} | \hat{\bm{b}}_{j}, \hat{V}_{j}, z_{j}=k,l) 
%p(z=k,l | \hat{\bm{b}}, \hat{V}, \hat{\bm{\pi} }),%v_{j}=1)
 %\\
\sim \sum_{p}^{P}  \tilde \pi_{jp} {\it N}_R (\bf{x};\bm{\tilde{\mu}}_{jp},\tilde{U} _{jp} )%,v_{j}=1) 


\end{aligned}
\end{equation}

Where again, ${\it N}_R(.; \bm{0}, \omega_l U_{p})$ denotes the density of a normal distribution with mean $\bm{\tilde{\mu}}_{p}$ and variance $\tilde{U} _{p}$ and the posterior mixture weight $\tilde \pi_{p}$ is simply 



 \begin{equation}
\tilde \pi_{jp} =\frac{ p(\hat{\bm{b}}_{j}| \hat{V}_{j}, z_{j}=p) \hat \pi_{p} } {\sum_{p=1}^{P} p(\hat{\bm{b}}_{j}| \hat{V}_{j}, z_{j}=p) \hat\pi_{p}}
 \label{eqn:postpi}
\end{equation}

Where $z_{j}=p$ is the latent variable indicator of the component identity and each $\hat\pi_{p}$ represents the Maximum Likelihood Estimate of the prior mixture weights assigned to each component (\ref{eq:pihat}).

\subsection{Reported Quantities}

For every gene-snp pair `j', we aim to report the effect size as the posterior mean, defined as:

\begin{equation}
\begin{aligned}
E(\bm{b}_{j} | \hat{\bm{b}_{j}}, \hat{V}_{j}, \hat{\bm{\pi}})
%&= \sum_{k=1,l=1}^{K,L} \sim {\it N}_R(\bm{\mu}_{1kl}, U_{1kl})%p(\bm{b}_{j} | \hat{\bm{b}}_{j}, \hat{V}_{j}, z_{j}=k,l) 
%p(z=k,l | \hat{\bm{b}}, \hat{V}, \hat{\bm{\pi} }),%v_{j}=1)
 %\\
= \sum_{p}^{P}  \tilde \pi_{p} \bm{\tilde{\mu}}_{p} %,v_{j}=1) 
\end{aligned}
\label{eq:mixmean}
\end{equation}


And the local false sign rate, or posterior probability of incorrectly identifying the sign of the effect for a given tissues `r' as :

\begin{equation}
  \label{eq:lfsr}
\begin{split}
P(b_{jr})= 1-max[{\sum_{p}}p({b_{j,r}}>0|\hat{\bm{b}}_{j}, \hat{V}_{j}, z_{j}=p)\tilde \pi_{p}, {\sum_{p}}p({b_{j,r}}<0|\hat{\bm{b}}_{j}, \hat{V}_{j}, z_{j}=p)\tilde \pi_{jp}]
\end{split}
\end{equation}

For the results section of this paper, we report the  posterior mean ($\ref{eq:mixmean}$) and LFSR ($\ref{eq:lfsr}$) for the `top' snp per gene, but armed with the hierarchical weights (\ref{eq:pihat}) and covariance matrices $\textbf{U}$ the posteriors can be computed for any gene-snp pair.

\section*{Supporting Information}

\subsection{Testing and Training}

In order to determine the optimal number and rank of the covariance matrices, we divide our data set into a training and test data set, each containing 8000 genes.

In the training set, we proceed as above: choosing the top SNP for each of the 8000 genes, creating a list of covariance matrices through deconvolution and grid selection of these top 'training gene-snp' pairs. 

Then, within the training data, we similarly choose a random set of gene-snp pairs (restricting our analysis to genes contained in the training set). Specifically, we choose 20,000 random-gene snp pairs and use the EM algorithm to learn the mixture proportions $\bm{\hat{\pi}}$  from this data set as in (\ref{eq:pihat}).

We then use the KxL vector of $\pi$ from the training set to estimate the log likelihood of each data point in the test data set. If our model is 'overfit' to the training data set, then a larger number of covariance matrices may actually decrease the test log-likelihood. 

\subsection{Visualizing All Patterns of Sharing}
\begin{figure}[htbp]
\includegraphics[width=10cm]{Figures/gtexresultcompiledheatmaps.png}
\caption{\textbf {Diverse Array of Relationships among tissues.} Here, we use heatmaps to visualize all the patterns of sharing present in the data}
\label{fig:allheat}
\end{figure}\newline

%In this data set, we find that K=1188 set of covariance matrices containing the Identity, the denoised empirical covariance matrix, rank five SFA approximation and rank 3 SVD approximation as well as 5 single-rank SFA factors and the 45 $eqtl.bma.lite$ configurations maximized this likelihood.

