---
title: "StandardError"
output: html_document
---

The aim of this document is to investigate the correlation of standard error and sample size, and to show how the presence of small sample sizes and large standard errors in biologically 'unique' tissues drives incompatibilities between the fold-size sharing heatmap and significance sharing heatmap.

```{r echo=FALSE,message=FALSE}
library('knitr')

knitr::opts_chunk$set(cache=TRUE)
opts_chunk$set(fig.path = "/Users/sarahurbut/Dropbox/PaperEdits/Paper/Figures/") 

data=read.csv("ExprSampleSize.csv.gz",header = T)
rownames(data)=data[,1]
expr.data=data[,-1]
maxz=read.table("../Data/maxz.txt")
qtl.names=sapply(1:length(rownames(maxz)),function(x){unlist(strsplit(rownames(maxz)[x], "[_]"))[[1]]})
expr.sort=expr.data[rownames(expr.data)%in%qtl.names,]


a=match(qtl.names,rownames(expr.sort))###in order to make sure the gene names are the same in both


expr.sort=expr.sort[a,]



missing.tissues=c(7,8,19,20,24,25,31,34,37)
exp.sort=expr.sort[,-missing.tissues]
colnames(exp.sort)=colnames(maxz)
standard.error=read.table("../Data/standard.error.txt")
colnames(standard.error)=colnames(maxz)

```

Look at the ordering of Sample Size and see how it is almost identical to that of standard error, though no sample sizes differ by more than about 4 fold. 

```{r, echo=FALSE}
size=apply(exp.sort,2,mean)
median.standard.error=apply(standard.error,2,median)
```

The correlation between the sqrt of the sample size and the median standard error is `r cor(sqrt(size),median.standard.error)`.



```{r sizebarplot, echo=FALSE}

#par(mfrow=c(1,2))
barplot(sort(size,decreasing=F),cex.names=0.5,main="SampleSize",names=colnames(maxz)[order(size,decreasing=F)],las=2)
barplot(sort(size,decreasing=T),cex.names=0.5,main="MedianStandardError",names=colnames(maxz)[order(median.standard.error,decreasing=T)],las=2)
```
 
 
Now note that the non 'system-group' (i.e., Gender like uterus, ovary and vagina, or brain, which exhibit similar effects based on biological intuition) tissues which show the strongest discrepancy between inconsistent significance and sharing by effect size are exactly those with smallest sample size: the Liver and Pituitary (not really part of the brain). So my intutition was correct: the lack of consistency in significance (as reflected by sparsity in the Jaccard Index Plot) in the presence of consistency in effect size is due to the small sample size of 'biologically unique' tissue. 

```{r standarderorranalysis3, echo=FALSE,message=FALSE}

library(gplots)
library(ggplot2)
library('colorRamps')

lfsr.mash=read.table("../../Dropbox/Aug12/Aug13withEDlfsr.txt")[,-1]
#lfsr.mash=as.matrix(read.table("withzeroomega2lfsr.txt")[,-1])
maxz=read.table("../../Dropbox/jul3/maxz.txt")
maxbeta=read.table("../Data/maxbetahats.txt")
tissue.names=colnames(maxz)
standard.error.from.z=as.matrix(maxbeta/maxz)##since we use the maxz to compute this model, we should use this standard error
pm.mash=as.matrix(read.table("../../Dropbox//Aug12/Aug13withEDposterior.means.txt")[,-1])
#pm.mash.omega=as.matrix(read.table("withzeroomega2posterior.means.txt")[,-1])
pm.mash.beta=pm.mash*standard.error.from.z



colnames(lfsr.mash)=colnames(pm.mash)=colnames(pm.mash.beta)=colnames(maxz)

weirdliverorder=read.table("../../Dropbox/weirdliverallorder.txt")[,1]

```



```{r standarderorranalysis2, echo=FALSE}
thresh=0.05

shared=matrix(NA,nrow = ncol(lfsr.mash),ncol=ncol(lfsr.mash))
colnames(shared)=rownames(shared)=colnames(maxz)

for(i in 1:ncol(lfsr.mash)){
  for(j in 1:ncol(lfsr.mash)){
    sig.row=which(lfsr.mash[,i]<thresh)
    sig.col=which(lfsr.mash[,j]<thresh)
    shared[i,j]=length(intersect(sig.row,sig.col))/length(union(sig.row,sig.col))
  }

}





thresh=0.05


shared.fold.size=matrix(NA,nrow = ncol(lfsr.mash),ncol=ncol(lfsr.mash))
colnames(shared.fold.size)=rownames(shared.fold.size)=colnames(maxz)
for(i in 1:ncol(lfsr.mash)){
  for(j in 1:ncol(lfsr.mash)){
    sig.row=which(lfsr.mash[,i]<thresh)
    sig.col=which(lfsr.mash[,j]<thresh)
    a=(union(sig.row,sig.col))
    #a=(intersect(sig.row,sig.col))
    #quotient=abs(pm.mash.beta[a,i]/pm.mash.beta[a,j])
    quotient=(pm.mash.beta[a,i]/pm.mash.beta[a,j])##divide effect sizes
    ##divide effect sizes
    #quotient=(pm.mash.ee[a,i]/pm.mash.ee[a,j])
    shared.fold.size[i,j]=mean(quotient>0.5&quotient<2)
   
  }}

heatmap.2(shared[rev(weirdliverorder),weirdliverorder],Rowv=FALSE,Colv=FALSE,
          symm=TRUE,dendrogram="none",density="none",trace="none",#col=redblue,
          col=blue2green,main=paste0("eQTL in Either that are Shared by Jaccard Index,EZ"),
          cexRow=0.5,cexCol=0.5,cex.main=0.5,breaks=seq(0.35,1,0.01))
mtext("Number Significant in Both/ Number Significant in Either")


heatmap.2(shared.fold.size[rev(weirdliverorder),weirdliverorder],Rowv=FALSE,Colv=FALSE,
          symm=TRUE,dendrogram="none",density="none",trace="none",#col=redblue,
          col=blue2green,main=paste0("eQTL in either tissues and within 2-fold,EZ"),
          cexRow=0.5,cexCol=0.5,cex.main=0.5,breaks=seq(0.35,1,0.01))
mtext("Number Within 2 fold/Number Significant in Either")
```

This difference persists with EE model as well (plots not shown for simplicity) which is somewhat reassuring from a methodological standpoint. The good news is, for the purpose of the paper, I think a little reordering of the tissues might fend off a referee :)

```{r standarderorranalysis,echo=FALSE}
all.tissue.order=read.table("../../Dropbox/alltissueorder.txt")[,1]

heatmap.2(shared[rev(all.tissue.order),all.tissue.order],Rowv=FALSE,Colv=FALSE,
          symm=TRUE,dendrogram="none",density="none",trace="none",#col=redblue,
          col=blue2green,main=paste0("eQTL in Either that are Shared by Jaccard,EZ"),
          cexRow=0.5,cexCol=0.5,cex.main=0.5,breaks=seq(0.35,1,0.01))


heatmap.2(shared.fold.size[rev(all.tissue.order),all.tissue.order],Rowv=FALSE,Colv=FALSE,
          symm=TRUE,dendrogram="none",density="none",trace="none",#col=redblue,
          col=blue2green,main=paste0("eQTL in either tissues and within 2-fold,EZ"),
          cexRow=0.5,cexCol=0.5,cex.main=0.5,breaks=seq(0.35,1,0.01))
```

Show the idea of how 'loner' tissues which show an unusual relationship between Effect and Significance tend to have a small sample size and median. standard. error.

```{r effectivesamplesizeplot}

#size.no.brain=sqrt(size[-c(7:16)])
#median.se.no.brain=median.standard.error[-c(7:16)]
median.se=median.standard.error
plot(median.se,1/sqrt(size),cex=0.1,ylab="1/sqrt(sample.size)",xlab="SE")
text(median.se,1/sqrt(size),colnames(maxz),cex=0.5,pos=2)
text(median.se["Liver"],1/sqrt(size)["Liver"],"Liver",col="red",cex=1.5,pos=2)
text(median.se["Pituitary"],1/sqrt(size)["Pituitary"],"Pituitary",col="blue",cex=1.5,pos=2)
```

And this translates to marginal variances:
```{r}
b.test=read.table("../Data/maxz.txt")
se.test=b.test/b.test
library('mvtnorm')
source("~/matrix_ash/R/main.R")
source("~/matrix_ash/R/mixEm.R")
j=14608  
r=38
k=1073
cov=readRDS("../Data/covmatAug13withED.rds")
pis=readRDS("../Data/pisAug13withED.rds")$pihat

all.arrays=post.array.per.snp(j=j,covmat = cov,b.gp.hat = b.test,se.gp.hat = se.test)

b.mle=as.vector(t(b.test[j,]))##turn i into a R x 1 vector
V.gp.hat=diag(se.test[j,])^2
V.gp.hat.inv <- solve(V.gp.hat)

log.lik.snp=log.lik.func(b.mle,V.gp.hat,cov)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
#log.pi=log(pis)
#s=log.lik.minus.max+log.pi
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))

U.gp1kl <- (post.b.gpkl.cov(V.gp.hat.inv, cov[[k]]))
mu.gp1kl <- as.array(post.b.gpkl.mean(b.mle, V.gp.hat.inv, U.gp1kl))

plot(all.arrays$post.covs[k,],diag(U.gp1kl))
##make sure individual posterior covariances are not greater than likliehood variance
sum(all.arrays$post.covs>1)
```

$Var(Y_{r})=E(Y_{r}^2)-E(Y_{r})^2$

So for $E(Y^2)$, we integrate over K, where Z is the random variable with $P(Z=k)=\tilde\pi_{k}$:

$E(Y_{r}^2)=E(E(Y_{r}^2|Z)$

$=\sum_{k}$ $\tilde\pi_{k}[U_{kr}+\mu_{kr}^2]$

So $Var(Y_{r})$ = $\sum_{k}$ $\tilde\pi_{k}[U_{kr}+\mu_{kr}^2]$ - $\sum_k \tilde\pi_{k} \mu_{kr}^2$

Let's check our calculations for a given tissue:

```{r marvarcheck, echo=TRUE}
marginal.var=read.table("../../Dropbox/Aug12/Aug13withEDmarginal.var.txt")[,-1]
#r=sample(seq(1:44),1)
r=38
pi.kl=post.weights


sum.test.part.one=post.weights*(all.arrays$post.covs[,r]+all.arrays$post.means[,r]^2)

##the majority (53% of wiehgt at componenet 1073)
####show example
diag(U.gp1kl)[r] ##less than 1
mu.gp1kl[r]^2

part.one.at.k=post.weights[k]*(all.arrays$post.covs[k,r]+all.arrays$post.means[k,r]^2)

mu.2.atk=(post.weights[k]*all.arrays$post.means[k,r])^2


sum.test.e.y.2.r=(post.weights*all.arrays$post.means[,r])

sum(sum.test.part.one)-sum(sum.test.e.y.2.r)^2


part.one.r=post.weights%*%(all.arrays$post.covs[,r]+all.arrays$post.means[,r]^2)
e.y.2.r=(post.weights%*%(all.arrays$post.means[,r]))^2
part.one.r-e.y.2.r


marginal.var[j,r]

##test for individula component
k=which.max(post.weights)


###show function correct for the first part acorss all tissues
part.one=post.weights%*%(all.arrays$post.covs+all.arrays$post.means^2)


plot(part.one,total.covs.partone.persnp(post.means = all.arrays$post.means,post.covs = all.arrays$post.covs,post.weights = post.weights))

####and check across all check with marginal variance computation##
e.y.2=(post.weights%*%(all.arrays$post.means))^2

plot(part.one-e.y.2,as.matrix(marginal.var[j,]))


part.one.tim=function(post.weights,post.covs){
  post.weights%*%post.covs
}

part.two.tim=function(post.weights,post.means,tissue){
  r=tissue
  grand.mean=post.weights%*%post.means
  post.weights%*%((post.means[,r]-grand.mean[r])^2)
}


b.test=read.table("../Data/maxz.txt")
se.test=b.test/b.test

j=14608  
r=38
k=1073

j=100
r=10

cov=readRDS("../Data/covmatAug13withED.rds")
pis=readRDS("../Data/pisAug13withED.rds")$pihat

all.arrays=post.array.per.snp(j=j,covmat = cov,b.gp.hat = b.test,se.gp.hat = se.test)

b.mle=as.vector(t(b.test[j,]))##turn i into a R x 1 vector
V.gp.hat=diag(se.test[j,])^2
V.gp.hat.inv <- solve(V.gp.hat)

log.lik.snp=log.lik.func(b.mle,V.gp.hat,cov)
log.lik.minus.max=log.lik.snp-max(log.lik.snp)
#log.pi=log(pis)
#s=log.lik.minus.max+log.pi
exp.vec=exp(log.lik.minus.max)
post.weights=t(exp.vec*pis/sum(exp.vec*pis))

part.one.tim(post.weights = post.weights,post.covs = all.arrays$post.covs)[r]
part.two.tim(post.weights = post.weights,post.means = all.arrays$post.means,tissue = r)

part.one.tim(post.weights = post.weights,post.covs = all.arrays$post.covs)[r]+part.two.tim(post.weights = post.weights,post.means = all.arrays$post.means,tissue = r)

total.covs.partone.persnp(all.arrays$post.means,all.arrays$post.covs,post.weights)[r]-((post.weights%*%all.arrays$post.means)^2)[r]

```


Now we look at the median poserior variances:
```{r effectivesamplesizemarvar}
median.mar.var=apply(marginal.var,2,median)
barplot(sort(1/median.mar.var,decreasing=F),cex.names=0.5,main="Effective Sample Size: 1/MedianMarginalVariance",names=colnames(maxz)[order(1/median.mar.var,decreasing=F)],las=2)
```

Now let's plot effective sample size. Recall:

$$n_{jeff}=\frac{s_{j}^2}{\tilde{s_{j}^2}}$$

```{r effectivesample}
#original.var=as.matrix(standard.error.from.z)^2
original.var=standard.error.from.z/standard.error.from.z
size=as.matrix(exp.sort)
post.var=as.matrix(marginal.var)#*standard.error.from.z^2
njeffective=size*original.var/post.var


##ask why some of the origianl variancres are smaller than posterior variances, even though calculations are right (e.g. 16012, 44)
###15093  44
increase=njeffective/size


barplot(sort(apply(increase,2,median),decreasing=F),las=2,cex.names=0.4)
title("Median(Nj_effective/Nj_original)")

barplot(sort(apply(njeffective,2,median),decreasing=F),cex.names=0.4,las=2)
title("MedianNj_effective")
```